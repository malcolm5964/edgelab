Here's the modified code to display all **21 hand landmarks** on the frame while detecting hands. This will help visualize the entire structure of the hand and observe the movement of fingers in real-time.  

### **Modifications**:
- **Loop through all 21 landmarks** for each detected hand.
- **Draw a circle at each landmark position**.
- **Draw connecting lines** to visualize the full hand structure.

---

### **Updated Code**
```python
#%% Reference: https://github.com/googlesamples/mediapipe/tree/main/examples/hand_landmarker/raspberry_pi
import cv2
import mediapipe as mp
from mediapipe.tasks import python
from mediapipe.tasks.python import vision

#%% Parameters
numHands = 2  # Number of hands to be detected
model = 'hand_landmarker.task'  # Model for finding hand landmarks
minHandDetectionConfidence = 0.5
minHandPresenceConfidence = 0.5
minTrackingConfidence = 0.5
frameWidth = 640
frameHeight = 480

# Visualization parameters
MARGIN = 10  # pixels
FONT_SIZE = 1
FONT_THICKNESS = 1
HANDEDNESS_TEXT_COLOR = (88, 205, 54)  # Vibrant green
POINT_COLOR = (0, 255, 0)  # Green for landmark points
LINE_COLOR = (255, 0, 0)  # Blue for landmark connections

#%% Create a HandLandmarker object
base_options = python.BaseOptions(model_asset_path=model)
options = vision.HandLandmarkerOptions(
    base_options=base_options,
    num_hands=numHands,
    min_hand_detection_confidence=minHandDetectionConfidence,
    min_hand_presence_confidence=minHandPresenceConfidence,
    min_tracking_confidence=minTrackingConfidence,
)
detector = vision.HandLandmarker.create_from_options(options)

#%% Open CV Video Capture
cap = cv2.VideoCapture(0)
cap.set(cv2.CAP_PROP_FRAME_WIDTH, frameWidth)
cap.set(cv2.CAP_PROP_FRAME_HEIGHT, frameHeight)

# Check if the webcam is opened correctly
if not cap.isOpened():
    raise IOError("Cannot open webcam")

# Mediapipe predefined hand connections
mp_hands = mp.solutions.hands

# Loop for video processing
while True:
    try:
        ret, frame = cap.read()
        if not ret:
            break
        
        frame = cv2.flip(frame, 1)  # Flip image for better visualization
        
        # Convert image format to RGB
        rgb_image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        
        # Run hand landmark detection
        mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=rgb_image)
        detection_result = detector.detect(mp_image)
        
        hand_landmarks_list = detection_result.hand_landmarks
        
        # Process each detected hand
        for hand_landmarks in hand_landmarks_list:
            points = []
            
            # Draw all 21 landmarks
            for idx, landmark in enumerate(hand_landmarks):
                x = int(landmark.x * frame.shape[1])
                y = int(landmark.y * frame.shape[0])
                points.append((x, y))
                
                # Draw landmark points
                cv2.circle(frame, (x, y), 5, POINT_COLOR, -1)
            
            # Draw landmark connections using Mediapipe's default hand connections
            for connection in mp_hands.HAND_CONNECTIONS:
                start_idx, end_idx = connection
                if start_idx < len(points) and end_idx < len(points):
                    cv2.line(frame, points[start_idx], points[end_idx], LINE_COLOR, 2)
            
            # Thumb Up detection
            thumb_tip_y = hand_landmarks[4].y
            thumb_base_y = hand_landmarks[1].y
            threshold = 0.1
            thumbs_up = thumb_tip_y < thumb_base_y - threshold
            
            if thumbs_up:
                cv2.putText(
                    frame, 'Thumb Up', (10, 30),
                    cv2.FONT_HERSHEY_DUPLEX, FONT_SIZE,
                    HANDEDNESS_TEXT_COLOR, FONT_THICKNESS, cv2.LINE_AA
                )
        
        # Display the annotated frame
        cv2.imshow('Hand Landmarks', frame)

        # Exit condition
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break

    except KeyboardInterrupt:
        break

# Release resources
cap.release()
cv2.destroyAllWindows()
```

---

### **Changes & Observations**
1. **Displays all 21 landmark points** with **green dots**.
2. **Connects the landmarks** using **blue lines** (Mediapipe's default hand structure).
3. **Maintains the "Thumb Up" detection**, allowing real-time hand pose analysis.
4. **Observing in real-time**:
   - Move your fingers and see how each landmark updates.
   - The **thumb, index, middle, ring, and pinky fingers** each have **4 points**.
   - The **palm base** is a reference point (landmark **0**).
   - You can visually inspect **finger bending and stretching**.

Try running the modified code and let me know if you need any refinements! ðŸš€










=====






Here's a Python script that performs **object detection-based video summarization**, extracting frames that contain a **specific object** (e.g., a **cellphone**) and saving them as a summarized video.  

---

### **Key Features:**
âœ… Detects objects in video frames using **EfficientDet (TFLite)**.  
âœ… Extracts and saves only frames **containing a specific object** (e.g., "cellphone").  
âœ… Saves the output as a **summarized video** with only relevant frames.

---

### **Code for Object Detection-Based Video Summarization**
```python
import cv2
import mediapipe as mp
import time

from mediapipe.tasks import python
from mediapipe.tasks.python import vision

#%% Parameters
maxResults = 5
scoreThreshold = 0.25
frameWidth = 640
frameHeight = 480
model = 'efficientdet.tflite'

# Object to filter (e.g., "cell phone")
TARGET_OBJECT = "cell phone"

# Video output settings
OUTPUT_VIDEO = "summarized_video.avi"
FRAME_RATE = 10  # Adjust FPS for output video

# Visualization parameters
MARGIN = 10
ROW_SIZE = 30
FONT_SIZE = 1
FONT_THICKNESS = 1
TEXT_COLOR = (0, 0, 0)  # Black for labels
BOX_COLOR = (0, 165, 255)  # Orange bounding box

#%% Initialize detection result storage
detection_result_list = []
filtered_frames = []

def save_result(result: vision.ObjectDetectorResult, unused_output_image: mp.Image, timestamp_ms: int):
    detection_result_list.append(result)

#%% Initialize the object detection model
base_options = python.BaseOptions(model_asset_path=model)
options = vision.ObjectDetectorOptions(
    base_options=base_options,
    running_mode=vision.RunningMode.LIVE_STREAM,
    max_results=maxResults,
    score_threshold=scoreThreshold,
    result_callback=save_result
)
detector = vision.ObjectDetector.create_from_options(options)

#%% Open video file or webcam
cap = cv2.VideoCapture("input_video.mp4")  # Change to 0 for webcam
cap.set(cv2.CAP_PROP_FRAME_WIDTH, frameWidth)
cap.set(cv2.CAP_PROP_FRAME_HEIGHT, frameHeight)

# Get video properties
frame_width = int(cap.get(3))
frame_height = int(cap.get(4))
fps = int(cap.get(cv2.CAP_PROP_FPS))

# Define video writer
fourcc = cv2.VideoWriter_fourcc(*'XVID')
out = cv2.VideoWriter(OUTPUT_VIDEO, fourcc, FRAME_RATE, (frame_width, frame_height))

# Check if the video is opened
if not cap.isOpened():
    raise IOError("Cannot open video file or webcam")

#%% Process video frames
while cap.isOpened():
    try:
        ret, frame = cap.read()
        if not ret:
            break  # Exit when video ends
        
        frame = cv2.flip(frame, 1)
        rgb_image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=rgb_image)
        current_frame = frame.copy()

        # Run object detection
        detector.detect_async(mp_image, time.time_ns() // 1_000_000)

        object_found = False  # Flag to check if target object is in frame

        if detection_result_list:
            for detection in detection_result_list[0].detections:
                bbox = detection.bounding_box
                start_point = (bbox.origin_x, bbox.origin_y)
                end_point = (bbox.origin_x + bbox.width, bbox.origin_y + bbox.height)

                category = detection.categories[0]
                category_name = category.category_name.lower()
                probability = round(category.score, 2)

                # Draw bounding box and label
                cv2.rectangle(current_frame, start_point, end_point, BOX_COLOR, 3)
                result_text = f"{category_name} ({probability})"
                text_location = (MARGIN + bbox.origin_x, MARGIN + ROW_SIZE + bbox.origin_y)
                cv2.putText(current_frame, result_text, text_location, cv2.FONT_HERSHEY_DUPLEX,
                            FONT_SIZE, TEXT_COLOR, FONT_THICKNESS, cv2.LINE_AA)

                # Check if detected object is the target
                if category_name == TARGET_OBJECT:
                    object_found = True
            
            detection_result_list.clear()

        # Save frame if target object is found
        if object_found:
            filtered_frames.append(current_frame)
            out.write(current_frame)  # Save to output video
            cv2.imshow('Filtered Video', current_frame)

        if cv2.waitKey(1) & 0xFF == ord('q'):
            break

    except KeyboardInterrupt:
        break

# Release resources
cap.release()
out.release()
cv2.destroyAllWindows()

print(f"Summarized video saved as {OUTPUT_VIDEO}")
```

---

### **How It Works**
1. **Reads video input** (`input_video.mp4`) or **webcam** (`cap = cv2.VideoCapture(0)`).
2. Uses **EfficientDet (TFLite)** for **real-time object detection**.
3. Filters frames containing the **specified object** (e.g., `"cell phone"`).
4. Saves only relevant frames into a **new summarized video** (`summarized_video.avi`).

---

### **Modifications You Can Make**
- Change `TARGET_OBJECT = "cell phone"` to filter a **different object**.
- Adjust `FRAME_RATE` for **smoother/faster output video**.
- Replace `"input_video.mp4"` with **camera input** (`0` for webcam).

---

### **Expected Output**
ðŸ”¹ If the video contains a **cellphone**, only those frames are saved into the **output video**.  
ðŸ”¹ **Unwanted frames** (without the object) are **discarded**, creating a **concise summary**.  

Let me know if you need modifications! ðŸš€
